{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2747b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefos/miniconda3/envs/env1/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/stefos/miniconda3/envs/env1/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu130\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Assuming 'biomarkers' is installed or on the python path\n",
    "from biomarkers.models.text import TextBiomarkerModel\n",
    "from biomarkers.core.base import BiomarkerConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c50c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    # --- Base Keys ---\n",
    "    'modality': 'text',\n",
    "    'model_type': 'TextBiomarkerModel', # Must match the class name potentially\n",
    "    'hidden_dim': 128,          # Smaller hidden dim for faster example\n",
    "    'num_diseases': 5,\n",
    "    'dropout': 0.1,\n",
    "    'device': device,           # Use detected device\n",
    "    # --- Custom Keys (will go into metadata) ---\n",
    "    'embedding_dim': 768,       # Dimension of input embeddings\n",
    "    'max_seq_length': 128,\n",
    "    'use_pretrained': False,    # Not loading a real pretrained model here\n",
    "    'pretrained_model': 'bert-base-uncased',\n",
    "    'biomarker_feature_dim': 50 # Dimension for aggregated biomarker vector\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c5fed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully!\n",
      "Model placed on device: cuda:0\n",
      "Number of parameters: 11,403,355\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = TextBiomarkerModel(config_dict)\n",
    "    print(\"Model initialized successfully!\")\n",
    "    print(f\"Model placed on device: {next(model.parameters()).device}\")\n",
    "    print(f\"Number of parameters: {model.num_parameters:,}\")\n",
    "    # print(\"\\nModel Config:\")\n",
    "    # print(model.config) \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing model: {e}\")\n",
    "    # If initialization fails, stop the notebook execution for this path\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47fd9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "print(\"\\nModel set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978eb574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy embeddings shape: torch.Size([4, 100, 768])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "seq_len = 100 # Should be <= max_seq_length, but model handles variable length\n",
    "embedding_dim = config_dict['embedding_dim']\n",
    "\n",
    "# Generate random embeddings tensor\n",
    "# The input tensor can be on CPU; the model's forward pass will move it to the correct device.\n",
    "dummy_embeddings = torch.randn(batch_size, seq_len, embedding_dim)\n",
    "\n",
    "print(f\"Dummy embeddings shape: {dummy_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1b49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata generated for 4 samples.\n"
     ]
    }
   ],
   "source": [
    "dummy_metadata = {\n",
    "    'tokens': [\n",
    "        ['this', 'is', 'sample', 'one', '.', 'it', 'has', 'um', 'pauses', '.'] * (seq_len // 10) \n",
    "        for _ in range(batch_size)\n",
    "    ],\n",
    "    'pos_tags': [\n",
    "        ['DT', 'VBZ', 'NN', 'CD', '.', 'PRP', 'VBZ', 'UH', 'NNS', '.'] * (seq_len // 10)\n",
    "        for _ in range(batch_size)\n",
    "    ],\n",
    "    'parse_trees': [\n",
    "        '(S (NP (DT this)) (VP (VBZ is) (NP (NN sample) (CD one))))' \n",
    "        for _ in range(batch_size)\n",
    "    ],\n",
    "    'timestamps': [\n",
    "        np.linspace(0, seq_len * 0.5, seq_len) \n",
    "        for _ in range(batch_size)\n",
    "     ],\n",
    "     'content_words': [\n",
    "         ['sample', 'one', 'pauses'] * (seq_len // 10)\n",
    "         for _ in range(batch_size)\n",
    "     ]\n",
    "}\n",
    "\n",
    "print(f\"Metadata generated for {len(dummy_metadata['tokens'])} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f49a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass completed.\n",
      "\n",
      "Output keys: dict_keys(['logits', 'probabilities', 'features', 'predictions', 'linguistic_pattern', 'biomarkers', 'cognitive_scores', 'clinical_scores', 'change_prediction', 'uncertainty', 'confidence'])\n",
      "\n",
      "--- Basic Outputs ---\n",
      "Logits shape: torch.Size([4, 5])\n",
      "Probabilities shape: torch.Size([4, 5])\n",
      "Predictions shape: torch.Size([4])\n",
      "Predictions: tensor([4, 4, 4, 4], device='cuda:0')\n",
      "Features (mean-pooled input) shape: torch.Size([4, 768])\n",
      "Linguistic Pattern Probs shape: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    output = model(\n",
    "        dummy_embeddings, \n",
    "        text_metadata=dummy_metadata,\n",
    "        return_biomarkers=True,\n",
    "        return_uncertainty=True,\n",
    "        return_clinical=True,\n",
    "        return_cognitive=True\n",
    "    )\n",
    "\n",
    "print(\"Forward pass completed.\")\n",
    "print(\"\\nOutput keys:\", output.keys())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Examine Outputs\n",
    "\n",
    "# %%\n",
    "print(\"\\n--- Basic Outputs ---\")\n",
    "print(f\"Logits shape: {output['logits'].shape}\") # (batch_size, num_diseases)\n",
    "print(f\"Probabilities shape: {output['probabilities'].shape}\") # (batch_size, num_diseases)\n",
    "print(f\"Predictions shape: {output['predictions'].shape}\") # (batch_size,)\n",
    "print(f\"Predictions: {output['predictions']}\")\n",
    "print(f\"Features (mean-pooled input) shape: {output['features'].shape}\") # (batch_size, embedding_dim)\n",
    "print(f\"Linguistic Pattern Probs shape: {output['linguistic_pattern'].shape}\") # (batch_size, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90921f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Biomarkers ---\n",
      "Number of biomarkers extracted: 29\n",
      "Examples (first sample):\n",
      "  - lexical_diversity: 0.4745 (shape: torch.Size([4]))\n",
      "  - vocabulary_richness: 0.5013 (shape: torch.Size([4]))\n",
      "  - semantic_diversity: 0.4974 (shape: torch.Size([4]))\n",
      "  - lexical_density: 0.3000 (shape: torch.Size([4]))\n",
      "  - syntactic_complexity: 0.5299 (shape: torch.Size([4]))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Biomarkers ---\")\n",
    "biomarkers = output['biomarkers']\n",
    "print(f\"Number of biomarkers extracted: {len(biomarkers)}\")\n",
    "# Print a few example biomarker values for the first sample\n",
    "print(\"Examples (first sample):\")\n",
    "for i, (key, value) in enumerate(biomarkers.items()):\n",
    "    if i < 5: # Print first 5\n",
    "         # Move to CPU for printing if needed\n",
    "        val_cpu = value[0].cpu().numpy() if isinstance(value, torch.Tensor) else value[0]\n",
    "        print(f\"  - {key}: {val_cpu:.4f} (shape: {value.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81724893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cognitive Scores ---\n",
      "Cognitive scores keys: dict_keys(['MMSE', 'MoCA', 'CDR'])\n",
      "MMSE scores shape: torch.Size([4])\n",
      "MMSE scores (first sample): 14.93\n",
      "MoCA scores shape: torch.Size([4])\n",
      "MoCA scores (first sample): 15.02\n",
      "CDR scores shape: torch.Size([4, 5])\n",
      "CDR scores (first sample): [0.18965001 0.20985328 0.2053401  0.2029706  0.19218606]\n",
      "\n",
      "--- Cognitive Scores ---\n",
      "Cognitive scores keys: dict_keys(['MMSE', 'MoCA', 'CDR'])\n",
      "MMSE scores shape: torch.Size([4])\n",
      "MMSE scores (first sample): 14.93\n",
      "MoCA scores shape: torch.Size([4])\n",
      "MoCA scores (first sample): 15.02\n",
      "CDR scores shape: torch.Size([4, 5])\n",
      "CDR scores (first sample): [0.18965001 0.20985328 0.2053401  0.2029706  0.19218606]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Cognitive Scores ---\")\n",
    "cognitive_scores = output['cognitive_scores']\n",
    "print(f\"Cognitive scores keys: {cognitive_scores.keys()}\")\n",
    "print(f\"MMSE scores shape: {cognitive_scores['MMSE'].shape}\") \n",
    "print(f\"MMSE scores (first sample): {cognitive_scores['MMSE'][0].item():.2f}\")\n",
    "print(f\"MoCA scores shape: {cognitive_scores['MoCA'].shape}\")\n",
    "print(f\"MoCA scores (first sample): {cognitive_scores['MoCA'][0].item():.2f}\")\n",
    "print(f\"CDR scores shape: {cognitive_scores['CDR'].shape}\") # (batch_size, 5)\n",
    "print(f\"CDR scores (first sample): {cognitive_scores['CDR'][0].cpu().numpy()}\")\n",
    "print(\"\\n--- Cognitive Scores ---\")\n",
    "cognitive_scores = output['cognitive_scores']\n",
    "print(f\"Cognitive scores keys: {cognitive_scores.keys()}\")\n",
    "print(f\"MMSE scores shape: {cognitive_scores['MMSE'].shape}\") \n",
    "print(f\"MMSE scores (first sample): {cognitive_scores['MMSE'][0].item():.2f}\")\n",
    "print(f\"MoCA scores shape: {cognitive_scores['MoCA'].shape}\")\n",
    "print(f\"MoCA scores (first sample): {cognitive_scores['MoCA'][0].item():.2f}\")\n",
    "print(f\"CDR scores shape: {cognitive_scores['CDR'].shape}\") # (batch_size, 5)\n",
    "print(f\"CDR scores (first sample): {cognitive_scores['CDR'][0].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f63e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Clinical Scores ---\n",
      "Clinical scores keys: dict_keys(['language_severity', 'decline_rate', 'communication_effectiveness'])\n",
      "Language Severity shape: torch.Size([4])\n",
      "Language Severity (first sample): 2.40\n",
      "Decline Rate shape: torch.Size([4])\n",
      "Decline Rate (first sample): 0.5174\n",
      "Communication Eff. shape: torch.Size([4])\n",
      "Communication Eff. (first sample): 4.93\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Clinical Scores ---\")\n",
    "clinical_scores = output['clinical_scores']\n",
    "print(f\"Clinical scores keys: {clinical_scores.keys()}\")\n",
    "print(f\"Language Severity shape: {clinical_scores['language_severity'].shape}\")\n",
    "print(f\"Language Severity (first sample): {clinical_scores['language_severity'][0].item():.2f}\")\n",
    "print(f\"Decline Rate shape: {clinical_scores['decline_rate'].shape}\")\n",
    "print(f\"Decline Rate (first sample): {clinical_scores['decline_rate'][0].item():.4f}\")\n",
    "print(f\"Communication Eff. shape: {clinical_scores['communication_effectiveness'].shape}\")\n",
    "print(f\"Communication Eff. (first sample): {clinical_scores['communication_effectiveness'][0].item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47909eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Uncertainty ---\n",
      "Uncertainty shape: torch.Size([4, 5])\n",
      "Confidence shape: torch.Size([4, 5])\n",
      "Confidence for predicted class (4) (first sample): 0.4830\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Uncertainty ---\")\n",
    "print(f\"Uncertainty shape: {output['uncertainty'].shape}\") # (batch_size, num_diseases)\n",
    "print(f\"Confidence shape: {output['confidence'].shape}\") # (batch_size, num_diseases)\n",
    "# Confidence for the predicted class for the first sample\n",
    "predicted_class_0 = output['predictions'][0].item()\n",
    "print(f\"Confidence for predicted class ({predicted_class_0}) (first sample): {output['confidence'][0, predicted_class_0].item():.4f}\")\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca26c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Change Prediction ---\n",
      "Change Prediction shape: torch.Size([4, 5])\n",
      "Change Prediction (first sample): [0.17957321 0.19120947 0.22037056 0.21395439 0.19489242]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Change Prediction ---\")\n",
    "print(f\"Change Prediction shape: {output['change_prediction'].shape}\") # (batch_size, 5)\n",
    "print(f\"Change Prediction (first sample): {output['change_prediction'][0].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77f4c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Output from model.predict() ---\n",
      "Keys: dict_keys(['logits', 'probabilities', 'features', 'predictions', 'linguistic_pattern', 'biomarkers', 'cognitive_scores', 'clinical_scores', 'change_prediction', 'uncertainty', 'confidence'])\n",
      "Predictions: tensor([4, 4, 4, 4], device='cuda:0')\n",
      "Confidence shape: torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions_output = model.predict(\n",
    "        dummy_embeddings, \n",
    "        # text_metadata=dummy_metadata, \n",
    "        return_confidence=True\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Output from model.predict() ---\")\n",
    "print(\"Keys:\", predictions_output.keys())\n",
    "print(f\"Predictions: {predictions_output['predictions']}\")\n",
    "print(f\"Confidence shape: {predictions_output['confidence'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c88628d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Output from model.get_biomarkers() ---\n",
      "Number of biomarkers: 29\n",
      "Keys: ['lexical_diversity', 'vocabulary_richness', 'semantic_diversity', 'lexical_density', 'syntactic_complexity', 'subordination_index', 'dependency_distance', 'grammar_accuracy', 'semantic_coherence', 'topic_consistency'] ...\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    biomarkers_only = model.get_biomarkers(dummy_embeddings)\n",
    "\n",
    "print(\"\\n--- Output from model.get_biomarkers() ---\")\n",
    "print(f\"Number of biomarkers: {len(biomarkers_only)}\")\n",
    "print(\"Keys:\", list(biomarkers_only.keys())[:10], \"...\") # Show first 10 keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d0ee95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"linguistic_decline_syndrome\": {\n",
      "    \"detected\": true,\n",
      "    \"severity\": \"moderate\",\n",
      "    \"composite_score\": \"0.50\",\n",
      "    \"clinical_note\": \"Multiple linguistic decline markers detected. Comprehensive cognitive evaluation recommended.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    biomarkers_sample_0 = model.extract_biomarkers(dummy_embeddings[0:1], {k: [v[0]] for k, v in dummy_metadata.items() if isinstance(v, list)})\n",
    "\n",
    "interpretation = model.get_clinical_interpretation(biomarkers_sample_0)\n",
    "print(json.dumps(interpretation, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14217dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Normative Comparison (First Sample, Age 65, Edu 12) ---\n",
      "{\n",
      "  \"lexical_diversity\": {\n",
      "    \"value\": 0.4745039939880371,\n",
      "    \"normative_mean\": 0.6,\n",
      "    \"z_score\": -1.0457999629496937,\n",
      "    \"interpretation\": \"mildly impaired\"\n",
      "  },\n",
      "  \"syntactic_complexity\": {\n",
      "    \"value\": 0.5299345254898071,\n",
      "    \"normative_mean\": 0.5499999999999999,\n",
      "    \"z_score\": -0.13376982114996397,\n",
      "    \"interpretation\": \"average\"\n",
      "  },\n",
      "  \"semantic_coherence\": {\n",
      "    \"value\": 0.5189153552055359,\n",
      "    \"normative_mean\": 0.7,\n",
      "    \"z_score\": -1.810846266860014,\n",
      "    \"interpretation\": \"moderately impaired\"\n",
      "  },\n",
      "  \"idea_density\": {\n",
      "    \"value\": 0.47677335143089294,\n",
      "    \"normative_mean\": 0.5,\n",
      "    \"z_score\": -0.1935553886129431,\n",
      "    \"interpretation\": \"average\"\n",
      "  },\n",
      "  \"information_content\": {\n",
      "    \"value\": 0.4920281767845154,\n",
      "    \"normative_mean\": 0.6499999999999999,\n",
      "    \"z_score\": -1.215167777413898,\n",
      "    \"interpretation\": \"mildly impaired\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# Compare to normative data (using biomarkers for the first sample)\n",
    "print(\"\\n--- Normative Comparison (First Sample, Age 65, Edu 12) ---\")\n",
    "comparison = model.compare_to_normative_data(biomarkers_sample_0, age=65, education=12)\n",
    "print(json.dumps(comparison, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afcb8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Predicting Trajectory over 3 time points ---\n",
      "Trajectory Class: stable\n",
      "Recommendation: Linguistic abilities remain stable. Continue routine monitoring as appropriate for age and risk factors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Longitudinal Trajectory Prediction (Example)\n",
    "\n",
    "# %%\n",
    "# Create dummy data for 3 time points\n",
    "num_time_points = 3\n",
    "longitudinal_embeddings = [torch.randn(1, seq_len, embedding_dim) for _ in range(num_time_points)]\n",
    "time_points_months = [0.0, 6.0, 12.0] # Example: Baseline, 6 months, 12 months\n",
    "\n",
    "print(f\"\\n--- Predicting Trajectory over {num_time_points} time points ---\")\n",
    "trajectory_output = model.predict_cognitive_trajectory(longitudinal_embeddings, time_points_months)\n",
    "\n",
    "print(f\"Trajectory Class: {trajectory_output.get('trajectory_class', 'N/A')}\")\n",
    "print(f\"Recommendation: {trajectory_output.get('recommendation', 'N/A')}\")\n",
    "# print(\"Metric Trends:\")\n",
    "# print(json.dumps(trajectory_output.get('metric_trends', {}), indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cae69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to temp_model_save/text_biomarker_model.pth\n",
      "\n",
      "Model loaded successfully!\n",
      "Loaded model device: cuda:0\n",
      "Logits match after loading: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Save and Load Model\n",
    "# \n",
    "# Use the `save` and `load` methods inherited from `BiomarkerModel`.\n",
    "\n",
    "# %%\n",
    "save_dir = Path(\"./temp_model_save\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "model_path = save_dir / \"text_biomarker_model.pth\"\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "print(f\"\\nModel saved to {model_path}\")\n",
    "\n",
    "# %%\n",
    "# Load the model\n",
    "try:\n",
    "    loaded_model = TextBiomarkerModel.load(model_path)\n",
    "    loaded_model.eval() # Set to eval mode after loading\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Loaded model device: {next(loaded_model.parameters()).device}\")\n",
    "\n",
    "    # Verify consistency (optional)\n",
    "    with torch.no_grad():\n",
    "        output_original = model(dummy_embeddings)\n",
    "        # Ensure input is on the correct device for the loaded model\n",
    "        output_loaded = loaded_model(dummy_embeddings.to(loaded_model.device)) \n",
    "    \n",
    "    # Compare logits (move loaded output back to CPU if necessary for comparison)\n",
    "    logits_match = torch.allclose(\n",
    "        output_original['logits'].cpu(), \n",
    "        output_loaded['logits'].cpu(), \n",
    "        atol=1e-6 # Add tolerance for potential float precision differences\n",
    "    )\n",
    "    print(f\"Logits match after loading: {logits_match}\")\n",
    "    assert logits_match\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568eed18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
